# Remember to delete the first row once the code is run
# The first row is a dummy row
leaderSummary <- data.frame(cbind(turk_id=0,
scale_name=scale_name,
estimate=0,
standard_error=0,
lowerCI=0,
upperCI=0))
lapply(leaderSummary, class)
leaderSummary[,-2] <- apply(leaderSummary[,-2], 2,
function(x) as.numeric(as.character(x)))
lapply(leaderSummary, class)
turk_rsp <- turk_responses[3,5:ncol(turk_responses)]
x <- as.matrix(turk_rsp)
table(x)
x <- x - 1
table(x)
y <- as.matrix(item_bank_master_for_pcm[,-c(1,2)])
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
thetaAfter
lowerCI <- min(item_bank$item_difficulty)
upperCI <- max(item_bank$item_difficulty)
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
thetaAfter
semTheta(thetaAfter,
y,
x,
"PCM", 1.702, "ML")
View(turk_responses)
View(estimate_fortran)
library(catR)
library(tidyr)
library(dplyr)
library(readr)
#Read the datasets
#The estimates of the turk.
#This data frame was generated from table 7.1.1 from the output file
estimate_fortran <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scan_turk.csv")
#the item bank
#This data frame was generated from table 7.5.1 from the output file
item_bank <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scanning_item.csv")
#rasch andrich threshold
#This data frame was generated from table 8.1 from the output file
rasch_andrich_threshold <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scan_andrich_thresholds.csv")
#Items that are to be considered to estimate ability
#Not all items in the item bank are used for computing the estimate.
#From the input file to the FORTRAN code, take all the items
#that aren't commented out (by semi-colon)
items_to_consider <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/items_for_computation.csv")
#Extract the first character of each row.
#If it is semi-colon, ignore it
items_to_consider$first_char <- apply(items_to_consider, 1, function(x) substr(as.character(x),1,1))
items_to_consider$item_num <- c(1:nrow(items_to_consider))
#Create the list of items that are to be considered
items_to_consider_list <- items_to_consider[items_to_consider$first_char!=";","item_num"]
#Turk master file
turk_master <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/turk_master.csv")
#The above file is redundant since
#the estimate_fortran has the num_id for each selected turk
#This file is just to cross-check if these two files match
temp1 <- estimate_fortran %>% left_join(turk_master, by="turk")
#Number of rows match. 293 in estimate_fortran.
#all turks found a match on turk_master.
#now- to check if the IDs match
sum(temp1$num_id - temp1$turk_id)
#sum is 0. All the ids matched
rm(temp1, turk_master)
scale_name="Environmental Scanning"
#One more check. To see if item numbers in estimate_foertran
#matches the items_to_consider_list
unique(!(item_bank$item_num %in% items_to_consider_list))
#Returns true. Both have 158 item lists and they are the same
#Load the responses
turk_responses <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/escan_responses.csv")
#Subset only those turk for whom estimates are to be done
turk_responses <- turk_responses[turk_responses$turk_id %in% estimate_fortran$num_id,  ]
#subset only those questions that are to be considered
items_to_consider_list <- as.list(items_to_consider_list)
turk_responses <- turk_responses[,c(1:4, items_to_consider_list$item_num+4)]
####################################
# End of reading inputs
####################################
####################################
#Data prep for PCM
####################################
item_bank_master_for_pcm <- item_bank[,c(3,1)]
#Sort the dataframe by question number
item_bank_master_for_pcm <- item_bank_master_for_pcm %>%
arrange(item_num)
#For input to PCM, need to add item difficulty and andrich thresholds
rasch_andrich_threshold <- data.frame(t(rasch_andrich_threshold))
rasch_andrich_threshold <- rasch_andrich_threshold[rep(seq_len(nrow(rasch_andrich_threshold)),
each=nrow(item_bank),),]
item_bank_master_for_pcm$delta1 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,1]
item_bank_master_for_pcm$delta2 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,2]
item_bank_master_for_pcm$delta3 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,3]
item_bank_master_for_pcm$delta4 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,4]
item_bank_master_for_pcm$delta5 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,5]
### Create a  data frame to store summary results for each turk_id
# Remember to delete the first row once the code is run
# The first row is a dummy row
leaderSummary <- data.frame(cbind(turk_id=0,
scale_name=scale_name,
estimate=0,
standard_error=0,
lowerCI=0,
upperCI=0))
lapply(leaderSummary, class)
leaderSummary[,-2] <- apply(leaderSummary[,-2], 2,
function(x) as.numeric(as.character(x)))
lapply(leaderSummary, class)
####################################
#Estimation of Ability for Turk
####################################
### Need to do loop once the initial test code is ready
for(turk_counter in 1:nrow(turk_responses)){
turk_rsp <- turk_responses[turk_counter,5:ncol(turk_responses)]
#Estimate for first item is computed. Then, a while loop for remaining items
#This is to create the first row of the computational process for each turk
response_counter <- 1
theta = 0
itemBankQuestionsAsked <- item_bank_master_for_pcm[1:response_counter,-c(1,2)]
itemResponseList <- as.vector(turk_rsp[1:response_counter])
lowerCI <- min(item_bank$item_difficulty)
upperCI <- max(item_bank$item_difficulty)
#Compute the updated theta now. Question was asked and
# the response was obtained
x <- as.matrix(turk_rsp[1:response_counter])
x <- x - 1
y <- as.matrix(item_bank_master_for_pcm[1:response_counter,-c(1,2)])
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
#print(thetaAfter)
#Compute standard error the estimate
std_error <- semTheta(thetaAfter,
y,
x,
"PCM", 1.702, "ML")
#print(std_error)
#Compute the updated confidence intervals
lowerCIUpdated <- thetaAfter - 1.702*std_error
upperCIUpdated <- thetaAfter + 1.702*std_error
print(lowerCIUpdated)
print(upperCIUpdated)
#Write the intermediate computation results to a dataframe
leaderComputationResults <- data.frame(cbind(turk_responses[turk_counter, 1],
scale_name =scale_name,
item_num = item_bank_master_for_pcm[response_counter,"item_num"],
item_difficulty = item_bank_master_for_pcm[response_counter,"item_difficulty"],
item_response = as.numeric(itemResponseList[response_counter]),
theta_before = theta,
theta_after = thetaAfter,
lower_ci = lowerCIUpdated,
upper_ci = upperCIUpdated,
standard_error = std_error
))
response_counter <- response_counter + 1
while(response_counter <= length(turk_rsp)){
print(response_counter)
theta <- thetaAfter
itemBankQuestionsAsked <- item_bank_master_for_pcm[1:response_counter,-c(1,2)]
itemResponseList <- as.vector(turk_rsp[1:response_counter])
#Compute the updated theta now. Question was asked and
# the response was obtained
x <- as.matrix(turk_rsp[1:response_counter])
x <- x - 1
y <- as.matrix(item_bank_master_for_pcm[1:response_counter,-c(1,2)])
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
#print(thetaAfter)
#Compute standard error the estimate
std_error <- semTheta(thetaAfter,
y,
x,
"PCM", 1.702, "ML")
#print(std_error)
#Compute the updated confidence intervals
lowerCIUpdated <- thetaAfter - 1.702*std_error
upperCIUpdated <- thetaAfter + 1.702*std_error
#print(lowerCIUpdated)
#print(upperCIUpdated)
#Write the intermediate computation results to a dataframe
leaderComputationResults <- data.frame(rbind(leaderComputationResults,
data.frame(cbind(turk_responses[turk_counter, 1],
scale_name =scale_name,
item_num = item_bank_master_for_pcm[response_counter,"item_num"],
item_difficulty = item_bank_master_for_pcm[response_counter,"item_difficulty"],
item_response = as.numeric(itemResponseList[response_counter]),
theta_before = theta,
theta_after = thetaAfter,
lower_ci = lowerCIUpdated,
upper_ci = upperCIUpdated,
standard_error = std_error
))))
response_counter <- response_counter + 1
}
leaderSummary <- data.frame(rbind(leaderSummary,
data.frame(cbind(turk_id=turk_responses[turk_counter, 1],
scale_name=scale_name,
estimate=thetaAfter,
standard_error=std_error,
lowerCI=lowerCIUpdated,
upperCI=upperCIUpdated))))
print(paste("done",turk_counter))
}
View(leaderSummary)
leaderSummary <- leaderSummary[-1, ]
View(leaderSummary)
View(estimate_fortran)
View(item_bank)
##########################################
View(item_bank)
View(estimate_fortran)
names(leaderSummary)
names(estimate_fortran)
ability_correlation <- merge(leaderSummary,
estimate_fortran[,-4],
by.x="turk_id",
by.y="num_id")
View(ability_correlation)
names(estimate_fortran)
names(estimate_fortran)[1:2] <- c("fortran_estimate", "fortran_std_err")
ability_correlation <- merge(leaderSummary,
estimate_fortran[,-4],
by.x="turk_id",
by.y="num_id")
View(ability_correlation)
?cor
cor(ability_correlation$estimate, ability_correlation$fortran_estimate)
?plot
plot(ability_correlation$estimate, ability_correlation$fortran_estimate)
write.csv(ability_correlation, "~/Documents/Kaggle/LeaderAmp/Working/catR_testResults_18Feb2016.csv", row.names=F)
?plot
onlineNews <- read.csv("~/Downloads/OnlineNewsPopularity/OnlineNewsPopularity.csv")
View(onlineNews)
table(onlineNews$n_tokens_content)
contentPivot <- table(onlineNews$n_tokens_content)
contentPivot <- data.frame(table(onlineNews$n_tokens_content))
View(contentPivot)
plot(contentPivot)
summary(onlineNews)
library(jsonlite)
data(mtcars)
mtcars
toJSON(mtcars)
mtcars_json <- toJSON(mtcars)
class(mtcars_json)
as.character(mtcars_json)
(mtcars_json)
temp <- mtcars_json
temp <- as.character(mtcars_json)
install.packages("Rserve")
Rserve()
library(Rserve)
Rserve()
Rserve()
Rserve('--no-save')
Rserve()
system(ls)
?system
Rserve()
Rserve(args='--no-save')
Rserve(args='--no-save')
libPaths()
Sys.getenv("RSTUDIO") == "1"
mtcars
x <- as.matrix(mtcars$cyl)
x
x - 1
x$x_1 <- x - 1
x$x_1 <- as.matrix(x - 1)
x <- as.matrix(mtcars$cyl)
x
mtcars$cyl
as.matrix(mtcars$cyl)
as.matrix(mtcars$cyl) - 1
row.names(mtcars)
row.names(mtcars) <- NULL
View(mtcars)
updatedItemBank <- c(3, 4, 5, 6, 7, 8)
updatedItemBank
itemsPrevSent <- c(4, 5)
itemsPrevSent
updatedItemBank %in% itemsPrevSent
!(updatedItemBank %in% itemsPrevSent)
updatedItemBank[!(updatedItemBank %in% itemsPrevSent)]
library(catR)
?nextItem
system(R)
system("R")
system("ls")
system("which R")
options(repos = c(CRAN = "http://cran.revolutionanalytics.com"))
install.packages("gunsales")
install.packages("gunsales")
install.packages(c("seasonal", "gunsales"))
install.packages("~/Downloads/gunsales_0.1.0.tar.gz", type="source")
install.packages("~/Downloads/gunsales_0.1.0.tar.gz", reops=NULL,type="source")
?load
load("~/Downloads/gunsales/data/alldata.RData")
load("~/Downloads/gunsales/data/poptotal.RData")
View(alldata)
View(poptotal)
View(alldata)
unique(alldata$multiplier)
unique(alldata$other)
version
options(repos = c(CRAN = "https://cran.cnr.berkeley.edu/"))
install.packages(c("seasonal", "gunsales"))
data(alldata)
?gunsales
df <- analysis()
library(gunsales)
df <- analysis()
View(df)
View(alldata)
class(alldata$guns_per_thousand)
class(alldata$population)
library(jsonlite)
data(mtcars)
mtcars
lapply(mtcars, class)
mtcars
mtcars[,11]
as.factor(mtcars[,11])
as.factor(mtcars[,11]) - 1
as.numeric(as.factor(mtcars[,11]))
as.numeric(as.factor(mtcars[,11])) -1
label <- as.numeric(as.factor(mtcars[,11])) -1
class(label)
as.numeric(as.factor(mtcars[,ncol(mtcars)])) -1
as.factor(as.numeric(as.factor(mtcars[,ncol(mtcars)])) -1)
as.factor(mtcars[,ncol(mtcars)] - 1)
as.factor(as.numeric(as.factor(mtcars[,ncol(mtcars])) -1)
as.factor(as.numeric(as.factor(mtcars[,ncol(mtcars)])) -1)
class(as.factor(as.numeric(as.factor(mtcars[,ncol(mtcars)])) -1))
setwd("~/Documents/Kaggle/bnp_paribas_claims")
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(xgboost)
train <- read.table("../data/train.csv", header=T, sep=",")
train <- read.table("data/train.csv", header=T, sep=",")
?read.table
train <- readr("data/train.csv", header=T, sep=",")
library(readr) # CSV file I/O, e.g. the read_csv function
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(xgboost)
train <- readr("data/train.csv", header=T, sep=",")
train <- read_csv("data/train.csv", header=T, sep=",")
train <- read_csv("data/train.csv")
y <- train[, 'target']
train <- train[, -2]
test <- read_csv("data/test.csv")
test <- read_csv("data/test.csv")
train[is.na(train)] <- -1
test[is.na(test)] <- -1
lapply(train, class)
f <- c()
for(i in 1:ncol(train)) {
if (is.character(train[, i])) f <- c(f, i)
}
f
is.character(train[,1])
is.character(train[,112])
names(train)
is.character(train[,113])
head(train[,113])
class(train)
train <- data.frame(train)
class(train)
test <- data.frame(test)
train <- data.frame(train)
f <- c()
for(i in 1:ncol(train)) {
if (is.character(train[, i])) f <- c(f, i)
}
f
for(i in 1:ncol(test)) {
if (is.character(test[, i])) f.t <- c(f.t, i)
}
f.t <- c()
for(i in 1:ncol(test)) {
if (is.character(test[, i])) f.t <- c(f.t, i)
}
f
f.t
head(train[,4])
head(as.numeric(train[,4]))
head(as.numeric(as.factor(train[,4])))
head(as.numeric(as.factor(train[,4])), 10)
head(train[,4], 10)
head(as.numeric(as.factor(train[,4])), 100)
head(train[,4], 100)
ttrain <- rbind(train, test)
for (i in f) {
ttrain[, i] <- as.numeric(as.factor(ttrain[, i]))
}
head(ttrain[,1])
head(ttrain[,112])
names(ttrain)
head(ttrain[,113])
head(train[,113])
train <- ttrain[1:nrow(train), ]
test <- ttrain[(nrow(train)+1):nrow(ttrain), ]
doTest <- function(y, train, test, param0, iter) {
n<- nrow(train)
xgtrain <- xgb.DMatrix(as.matrix(train), label = y)
xgval = xgb.DMatrix(as.matrix(test))
watchlist <- list('train' = xgtrain)
model = xgb.train(
nrounds = iter
, params = param0
, data = xgtrain
, watchlist = watchlist
, print.every.n = 100
, nthread = 8
)
p <- predict(model, xgval)
rm(model)
gc()
p
}
param0 <- list(
# general , non specific params - just guessing
"objective"  = "binary:logistic"
, "eval_metric" = "logloss"
, "eta" = 0.01
, "subsample" = 0.8
, "colsample_bytree" = 0.8
, "min_child_weight" = 1
, "max_depth" = 10
)
submission <- read.table("data/sample_submission.csv", header=TRUE, sep=',')
ensemble <- rep(0, nrow(test))
p <- doTest(y, train, test, param0, 100)
exit
exit()
quit
quit()
y
length(y)
class(y)
y <- data.frame(y)
p <- doTest(y, train, test, param0, 100)
dim(y)
dim(train)
dim(test)
dim(y)
head(y)
p <- doTest(y$target, train, test, param0, 100)
ensemble <- ensemble + p
dim(ensemble)
length(ensemble)
head(ensemble)
ensemble <- rep(0, nrow(test))
for (i in 1:5) {
p <- doTest(y$target, train, test, param0, 400)
# change to 1300 or 1200, test by trial and error, have to add to local check which suggests 900,
# but have another 20% training data to concider which gives longer optimal training time
ensemble <- ensemble + p
}
submission$PredictedProb <- ensemble/i
write.csv(submission, "submission/submission.csv", row.names=F, quote=F)
?control
?Control
x <- 1
if(x>1){
print("X is greater than 1")
} else {
print("X is less than or equal to 1")
}
x <- 1
if(x>1){
print(paste("X is greater than 1 and its value is:", X))
} else {
print(paste("X is less than 1 and its value is:", X))
}
x <- 1
if(x>1){
print(paste("X is greater than 1 and its value is:", x))
} else {
print(paste("X is less than 1 and its value is:", x))
}
x <- 2
if(x>1){
print(paste("X is greater than 1 and its value is:", x))
} else {
print(paste("X is less than 1 and its value is:", x))
}
print(x)
print("this is hi")
library(catR)
?nextItem
